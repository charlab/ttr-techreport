\section{Introduction}

Caching is used in processor design to keep data close to the
computational units, which improves memory access latency, and reduces off-chip memory bandwidth.  Multi-level
caches store data in progressively larger,
and further away structures.  The increased distance, and especially
the increased size, of higher levels of cache mean that the higher
levels of cache have longer latencies to access.  If all of the
on-processor caches fail to contain the data being accessed by the
processor right now, then a long latency and high energy access to
main memory is used to fetch the data into the caches so it will be
available for the next time that data is accessed.

The Last Level Cache (LLC) is especially important for performance
because it represents the last opportunity for a memory access to
remain on-chip, without having to fetch data from the DRAM main
memory.  While an LLC cache hit may take 10x more cycles to complete
than an L1 hit, an LLC miss takes an additional 10x longer to
service.  An LLC miss not only takes longer to complete, but
activating the
DRAM system also uses more energy to service the request.  From an
energy perspective, an LLC hit consumes only the energy to service the
memory request by the LLC.  On the other hand, an LLC miss requires
the energy to
check the LLC, notice that it's a miss, and then fetch the data from
DRAM, possibly also needing to write back dirty data to DRAM which has been
displaced by the incoming data. 

In an effort to improve the effectiveness of the LLC and reduce the
number of DRAM accesses, a number of cache management policies have
been proposed, often based around having a set associative cache, such
as the Least Recently Used (LRU) policy, Not Recently Used
% could use citation
(NRU) policy, and others.  Each of these cache management policies
attempts to
evict data that is no longer deemed useful in favor of newly accessed
data which was not found in the LLC before.  Recently the
Re-reference Interval Prediction (RRIP)~\cite{jaleeltheobald10} cache
management policy has
been proposed as a way to improve LLC performance by attempting to
predict the future usefulness of cache blocks.  RRIP is represented by
two distinct varieties, namely Static RRIP (SRRIP) and Dynamic RRIP (DRRIP), which
each use a different insertion policy when dealing with incoming
cache blocks.  Both SRRIP and DRRIP are discussed at length in section
\ref{sec:policies}.

In this paper we explore the effectiveness of the Time to Recache
(TTR) metric in offering insight into why different cache management
policies perfor better than others.  This metric refers
to the time spent by a cache line after it has been evicted from the
LLC and before it is fetched again.  It is related to, but distinct
from, the notion of ``reuse distance,'' which refers to the amount of
time between successive accesses to a given cache block.
